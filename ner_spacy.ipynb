{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Customer NER model in sapcy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a spacy model and chekc if it has ner\n",
    "import spacy\n",
    "# nlp=spacy.load('en_core_web_sm')\n",
    "nlp=spacy.load('xx_ent_wiki_sm')\n",
    "\n",
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the pipeline component\n",
    "ner=nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text='AUTOMATIC DRAWING, 2018004228000489 AMERICAN EXPRESS SOL PRODUCTS PTY and INTER-BANK CREDIT, TonicLane0722 TONIC LANE PTY L SOL Products Pty'\n",
    "doc=nlp(article_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA = [\n",
    "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
    "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
    "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
    "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
    "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
    "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
    "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
    "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
    "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
    "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
    "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
    "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
    "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
    "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding labels to the `ner`\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable pipeline components you dont need to change\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the NER model\n",
    "\n",
    "First, let’s understand the ideas involved before going to the code.\n",
    "\n",
    "(a) To train an ner model, the model has to be looped over the example for sufficient number of iterations. If you train it for like just 5 or 6 iterations, it may not be effective.\n",
    "\n",
    "(b) Before every iteration it’s a good practice to shuffle the examples randomly throughrandom.shuffle() function .\n",
    "\n",
    "This will ensure the model does not make generalizations based on the order of the examples.\n",
    "\n",
    "(c) The training data is usually passed in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requirements\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "  # Training for 30 iterations\n",
    "  for iteration in range(30):\n",
    "\n",
    "    # shuufling examples  before every iteration\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "doc = nlp(\"I was driving a Alto\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the  model to directory\n",
    "output_dir = Path('/content/')\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)\n",
    "\n",
    "# Load the saved model and predict\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp_updated = spacy.load(output_dir)\n",
    "doc = nlp_updated(\"Fridge can be ordered in FlipKart\" )\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NER from a blank spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "\n",
    "nlp.add_pipe(nlp.create_pipe('ner'))\n",
    "\n",
    "nlp.begin_training()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "follow the same exact procedure as in the case for pre-existing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training completely new entity type in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and load the spacy model\n",
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_lg\") \n",
    "\n",
    "# Getting the ner component\n",
    "ner=nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New label to add\n",
    "LABEL = \"FOOD\"\n",
    "\n",
    "# Training examples in the required format\n",
    "TRAIN_DATA =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"Pasta is an italian recipe\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"China's noodles are very famous\", {\"entities\": [(8,14, \"FOOD\")]}),\n",
    "              (\"Shrimps are famous in China too\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Udon is a healthy type of noodles\", {\"entities\": [(0,4, \"ORG\")]}),\n",
    "              (\"Chocolate soufflé is extremely famous french cuisine\", {\"entities\": [(0,17, \"FOOD\")]}),\n",
    "              (\"Flamiche is french pastry\", {\"entities\": [(0,8, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, \"FOOD\")]})\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new label to ner\n",
    "ner.add_label(LABEL)\n",
    "\n",
    "# Resume training\n",
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "\n",
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 8.30617344158236}\n",
      "Losses {'ner': 12.530081017935686}\n",
      "Losses {'ner': 17.72781226175987}\n",
      "Losses {'ner': 23.5134511220994}\n",
      "Losses {'ner': 30.142971201249953}\n",
      "Losses {'ner': 34.29515378874374}\n",
      "Losses {'ner': 39.704931962831104}\n",
      "Losses {'ner': 45.641172330547995}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepak/opt/anaconda3/envs/mynlp/lib/python3.7/site-packages/spacy/language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"China's noodles are very famous\" with entities \"[(8, 14, 'FOOD')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 49.80742558260477}\n",
      "Losses {'ner': 56.57269441419681}\n",
      "Losses {'ner': 60.712227677850144}\n",
      "Losses {'ner': 65.41609042998618}\n",
      "Losses {'ner': 70.69118044734216}\n",
      "Losses {'ner': 75.31106345738965}\n",
      "Losses {'ner': 4.8581863339059055}\n",
      "Losses {'ner': 12.068656334942233}\n",
      "Losses {'ner': 17.667504258829894}\n",
      "Losses {'ner': 20.892584238111766}\n",
      "Losses {'ner': 25.709184050470384}\n",
      "Losses {'ner': 31.60164972176196}\n",
      "Losses {'ner': 34.98258298408243}\n",
      "Losses {'ner': 37.87771972886421}\n",
      "Losses {'ner': 39.91545630158291}\n",
      "Losses {'ner': 40.750114909988866}\n",
      "Losses {'ner': 45.74044731879704}\n",
      "Losses {'ner': 53.69873697185986}\n",
      "Losses {'ner': 59.002971100305245}\n",
      "Losses {'ner': 62.689515257422414}\n",
      "Losses {'ner': 5.08629464515252}\n",
      "Losses {'ner': 9.649530124937883}\n",
      "Losses {'ner': 18.829942357813707}\n",
      "Losses {'ner': 22.669617636565818}\n",
      "Losses {'ner': 29.574781550531043}\n",
      "Losses {'ner': 30.35448184303823}\n",
      "Losses {'ner': 34.72214235176216}\n",
      "Losses {'ner': 43.98967243541847}\n",
      "Losses {'ner': 47.33879455385977}\n",
      "Losses {'ner': 50.59494199345136}\n",
      "Losses {'ner': 57.47905297260513}\n",
      "Losses {'ner': 58.7144457312279}\n",
      "Losses {'ner': 62.076638772708975}\n",
      "Losses {'ner': 65.7275664765366}\n",
      "Losses {'ner': 6.180294752120972}\n",
      "Losses {'ner': 10.631143856793642}\n",
      "Losses {'ner': 14.871909422799945}\n",
      "Losses {'ner': 19.45590117480606}\n",
      "Losses {'ner': 24.33073886204511}\n",
      "Losses {'ner': 27.329644535668194}\n",
      "Losses {'ner': 31.527782413177192}\n",
      "Losses {'ner': 33.97094449959695}\n",
      "Losses {'ner': 36.933786504145246}\n",
      "Losses {'ner': 39.29680105409352}\n",
      "Losses {'ner': 42.52410089815385}\n",
      "Losses {'ner': 46.706305304862326}\n",
      "Losses {'ner': 51.04640798395849}\n",
      "Losses {'ner': 57.14815036705113}\n",
      "Losses {'ner': 6.455632988363504}\n",
      "Losses {'ner': 13.30214119143784}\n",
      "Losses {'ner': 16.32286680676043}\n",
      "Losses {'ner': 16.976604620344006}\n",
      "Losses {'ner': 20.449678648612462}\n",
      "Losses {'ner': 24.99830047262367}\n",
      "Losses {'ner': 28.09245082642883}\n",
      "Losses {'ner': 34.389096586965024}\n",
      "Losses {'ner': 39.45026634540409}\n",
      "Losses {'ner': 43.18528217682615}\n",
      "Losses {'ner': 46.27023053029552}\n",
      "Losses {'ner': 51.81500173127279}\n",
      "Losses {'ner': 53.57307145697996}\n",
      "Losses {'ner': 55.74144208012149}\n",
      "Losses {'ner': 1.145929291844368}\n",
      "Losses {'ner': 6.041350465966389}\n",
      "Losses {'ner': 6.182590051088482}\n",
      "Losses {'ner': 12.45397701847105}\n",
      "Losses {'ner': 13.568750879312574}\n",
      "Losses {'ner': 14.82582897820248}\n",
      "Losses {'ner': 16.877415559094516}\n",
      "Losses {'ner': 18.32238981850969}\n",
      "Losses {'ner': 23.983874991201446}\n",
      "Losses {'ner': 31.28988132964878}\n",
      "Losses {'ner': 35.279614348561154}\n",
      "Losses {'ner': 36.00141546908708}\n",
      "Losses {'ner': 40.94744586046727}\n",
      "Losses {'ner': 47.020708820506115}\n",
      "Losses {'ner': 2.683162788071968}\n",
      "Losses {'ner': 4.716110451675377}\n",
      "Losses {'ner': 8.802525356358274}\n",
      "Losses {'ner': 12.764615144905292}\n",
      "Losses {'ner': 13.821960605421737}\n",
      "Losses {'ner': 19.05207171049824}\n",
      "Losses {'ner': 20.03021467948838}\n",
      "Losses {'ner': 20.18351580073886}\n",
      "Losses {'ner': 26.473420694987908}\n",
      "Losses {'ner': 33.02809397706824}\n",
      "Losses {'ner': 37.4991491454939}\n",
      "Losses {'ner': 42.518077359470226}\n",
      "Losses {'ner': 47.35371888690224}\n",
      "Losses {'ner': 54.07125191239993}\n",
      "Losses {'ner': 6.015932433768285}\n",
      "Losses {'ner': 15.2977675129614}\n",
      "Losses {'ner': 19.024726809550884}\n",
      "Losses {'ner': 23.65570307670987}\n",
      "Losses {'ner': 26.434005008627537}\n",
      "Losses {'ner': 28.51982297477207}\n",
      "Losses {'ner': 34.06214076109882}\n",
      "Losses {'ner': 34.093295212263}\n",
      "Losses {'ner': 37.321816727878286}\n",
      "Losses {'ner': 37.46781043301871}\n",
      "Losses {'ner': 41.055610832186176}\n",
      "Losses {'ner': 45.895695804482784}\n",
      "Losses {'ner': 52.34158426416525}\n",
      "Losses {'ner': 56.968247073567}\n",
      "Losses {'ner': 2.7730315956396225}\n",
      "Losses {'ner': 7.649627450398839}\n",
      "Losses {'ner': 8.979161020612082}\n",
      "Losses {'ner': 13.176016575900576}\n",
      "Losses {'ner': 15.241935911075416}\n",
      "Losses {'ner': 18.567029147528956}\n",
      "Losses {'ner': 19.611971236949557}\n",
      "Losses {'ner': 22.283637723862284}\n",
      "Losses {'ner': 26.346730307652706}\n",
      "Losses {'ner': 32.71027285417085}\n",
      "Losses {'ner': 35.21962962047701}\n",
      "Losses {'ner': 42.9872289286418}\n",
      "Losses {'ner': 46.897639467452336}\n",
      "Losses {'ner': 49.66359831306772}\n",
      "Losses {'ner': 5.958814607773093}\n",
      "Losses {'ner': 8.171134025629726}\n",
      "Losses {'ner': 14.305242632355657}\n",
      "Losses {'ner': 17.95864861163136}\n",
      "Losses {'ner': 21.70378397952902}\n",
      "Losses {'ner': 27.317086996400235}\n",
      "Losses {'ner': 33.436193760970696}\n",
      "Losses {'ner': 41.78829490814553}\n",
      "Losses {'ner': 48.444607362637385}\n",
      "Losses {'ner': 56.37219035394685}\n",
      "Losses {'ner': 60.458965366494}\n",
      "Losses {'ner': 65.09395094753438}\n",
      "Losses {'ner': 68.64517532454067}\n",
      "Losses {'ner': 70.97098419607812}\n",
      "Losses {'ner': 4.047672139189672}\n",
      "Losses {'ner': 7.146449670835864}\n",
      "Losses {'ner': 12.103507444902789}\n",
      "Losses {'ner': 17.87631726328982}\n",
      "Losses {'ner': 23.58264967021387}\n",
      "Losses {'ner': 25.80495724413049}\n",
      "Losses {'ner': 30.08552163148852}\n",
      "Losses {'ner': 32.31690080890985}\n",
      "Losses {'ner': 36.34851117486687}\n",
      "Losses {'ner': 39.40590034561319}\n",
      "Losses {'ner': 42.963812910915294}\n",
      "Losses {'ner': 43.131099537909904}\n",
      "Losses {'ner': 46.5406573423752}\n",
      "Losses {'ner': 48.12674412663182}\n",
      "Losses {'ner': 4.0889594331383705}\n",
      "Losses {'ner': 7.9444735907018185}\n",
      "Losses {'ner': 12.714030113071203}\n",
      "Losses {'ner': 21.353279314935207}\n",
      "Losses {'ner': 24.850943098776042}\n",
      "Losses {'ner': 28.179488320834935}\n",
      "Losses {'ner': 32.78192717279671}\n",
      "Losses {'ner': 38.10926240147455}\n",
      "Losses {'ner': 38.1736533850235}\n",
      "Losses {'ner': 42.197855225185776}\n",
      "Losses {'ner': 47.13990571198883}\n",
      "Losses {'ner': 49.597198819745245}\n",
      "Losses {'ner': 50.56767359096921}\n",
      "Losses {'ner': 53.01765004800836}\n",
      "Losses {'ner': 2.2053300300613046}\n",
      "Losses {'ner': 7.848437801934779}\n",
      "Losses {'ner': 9.241077663252895}\n",
      "Losses {'ner': 15.048943714114671}\n",
      "Losses {'ner': 19.84695716282988}\n",
      "Losses {'ner': 21.79876625881991}\n",
      "Losses {'ner': 23.38614901105359}\n",
      "Losses {'ner': 27.386785502650696}\n",
      "Losses {'ner': 35.09202087455526}\n",
      "Losses {'ner': 39.757534575202385}\n",
      "Losses {'ner': 45.71433522021232}\n",
      "Losses {'ner': 47.96927481020464}\n",
      "Losses {'ner': 53.28366351001603}\n",
      "Losses {'ner': 56.3953814715627}\n",
      "Losses {'ner': 6.575377270579338}\n",
      "Losses {'ner': 9.420508394541685}\n",
      "Losses {'ner': 13.500590622948948}\n",
      "Losses {'ner': 17.103850410530868}\n",
      "Losses {'ner': 17.106772872310927}\n",
      "Losses {'ner': 19.58313230766808}\n",
      "Losses {'ner': 22.726939516121007}\n",
      "Losses {'ner': 25.93259844625291}\n",
      "Losses {'ner': 28.141179985327568}\n",
      "Losses {'ner': 33.61069527543887}\n",
      "Losses {'ner': 39.10531470513243}\n",
      "Losses {'ner': 41.194764690085236}\n",
      "Losses {'ner': 47.11327471684936}\n",
      "Losses {'ner': 48.098247504643865}\n",
      "Losses {'ner': 0.0726544072094839}\n",
      "Losses {'ner': 2.7966846229873568}\n",
      "Losses {'ner': 7.281538185281931}\n",
      "Losses {'ner': 10.532935256688177}\n",
      "Losses {'ner': 12.549469395189874}\n",
      "Losses {'ner': 18.382771588598473}\n",
      "Losses {'ner': 20.10054714532299}\n",
      "Losses {'ner': 24.713719001533093}\n",
      "Losses {'ner': 29.05674599311743}\n",
      "Losses {'ner': 29.05724777427116}\n",
      "Losses {'ner': 29.05727987714338}\n",
      "Losses {'ner': 32.94870216199772}\n",
      "Losses {'ner': 35.01992506300369}\n",
      "Losses {'ner': 37.97141569712469}\n",
      "Losses {'ner': 2.585453471081564}\n",
      "Losses {'ner': 8.55467298101965}\n",
      "Losses {'ner': 15.094324970956478}\n",
      "Losses {'ner': 21.97614296726195}\n",
      "Losses {'ner': 29.423874875672738}\n",
      "Losses {'ner': 32.972438021734234}\n",
      "Losses {'ner': 38.08691470202419}\n",
      "Losses {'ner': 43.27918935002188}\n",
      "Losses {'ner': 49.666416400981944}\n",
      "Losses {'ner': 53.011738470228266}\n",
      "Losses {'ner': 55.429536879986955}\n",
      "Losses {'ner': 60.86082309204383}\n",
      "Losses {'ner': 65.953200799406}\n",
      "Losses {'ner': 72.19240559914786}\n",
      "Losses {'ner': 4.714591229567304}\n",
      "Losses {'ner': 9.233452908461913}\n",
      "Losses {'ner': 12.92491911817342}\n",
      "Losses {'ner': 15.149073249602225}\n",
      "Losses {'ner': 15.150361518361024}\n",
      "Losses {'ner': 18.34801666809426}\n",
      "Losses {'ner': 20.547700669977985}\n",
      "Losses {'ner': 25.17223638274072}\n",
      "Losses {'ner': 29.599155798333868}\n",
      "Losses {'ner': 36.903083597613204}\n",
      "Losses {'ner': 41.08840767231368}\n",
      "Losses {'ner': 44.51804795851103}\n",
      "Losses {'ner': 49.15509297144354}\n",
      "Losses {'ner': 52.36150615436827}\n",
      "Losses {'ner': 4.598405133932829}\n",
      "Losses {'ner': 9.701425781357102}\n",
      "Losses {'ner': 13.945347561690141}\n",
      "Losses {'ner': 20.17309553847008}\n",
      "Losses {'ner': 24.01128200192352}\n",
      "Losses {'ner': 28.667613910346972}\n",
      "Losses {'ner': 33.75097600913023}\n",
      "Losses {'ner': 35.97813459399504}\n",
      "Losses {'ner': 42.31479523244661}\n",
      "Losses {'ner': 44.28024507812688}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 47.81120741679797}\n",
      "Losses {'ner': 49.998184506238886}\n",
      "Losses {'ner': 56.03143452705581}\n",
      "Losses {'ner': 56.05134708199648}\n",
      "Losses {'ner': 3.552627828706136}\n",
      "Losses {'ner': 9.815679528203873}\n",
      "Losses {'ner': 12.44717051031239}\n",
      "Losses {'ner': 15.961388393660854}\n",
      "Losses {'ner': 24.025820686837505}\n",
      "Losses {'ner': 27.032415095141005}\n",
      "Losses {'ner': 32.42840884195812}\n",
      "Losses {'ner': 36.719276268390786}\n",
      "Losses {'ner': 38.70634375400459}\n",
      "Losses {'ner': 43.904861564557905}\n",
      "Losses {'ner': 48.70887808017051}\n",
      "Losses {'ner': 53.27907414994752}\n",
      "Losses {'ner': 53.525644633030424}\n",
      "Losses {'ner': 55.563655925375315}\n",
      "Losses {'ner': 6.286850154399872}\n",
      "Losses {'ner': 9.474317088606767}\n",
      "Losses {'ner': 15.380708949523978}\n",
      "Losses {'ner': 21.003805724089034}\n",
      "Losses {'ner': 22.761416374965847}\n",
      "Losses {'ner': 24.787672455682696}\n",
      "Losses {'ner': 26.205480833797992}\n",
      "Losses {'ner': 30.017266762703002}\n",
      "Losses {'ner': 33.17392391402791}\n",
      "Losses {'ner': 35.171592302256045}\n",
      "Losses {'ner': 39.90604280271452}\n",
      "Losses {'ner': 47.02041768828292}\n",
      "Losses {'ner': 51.2705588507082}\n",
      "Losses {'ner': 53.53255431952073}\n",
      "Losses {'ner': 6.059158395975828}\n",
      "Losses {'ner': 10.633767079561949}\n",
      "Losses {'ner': 14.179470420749567}\n",
      "Losses {'ner': 22.832178891570948}\n",
      "Losses {'ner': 24.618026505195303}\n",
      "Losses {'ner': 27.412618374357862}\n",
      "Losses {'ner': 30.90821226996968}\n",
      "Losses {'ner': 35.41824278161994}\n",
      "Losses {'ner': 39.67021834998059}\n",
      "Losses {'ner': 43.53262300937104}\n",
      "Losses {'ner': 49.40353109090256}\n",
      "Losses {'ner': 53.62649984619247}\n",
      "Losses {'ner': 60.34735195369785}\n",
      "Losses {'ner': 63.269579328907184}\n",
      "Losses {'ner': 3.904603261500597}\n",
      "Losses {'ner': 3.9178156183879764}\n",
      "Losses {'ner': 8.787489965729037}\n",
      "Losses {'ner': 12.240198865922139}\n",
      "Losses {'ner': 16.56153263190572}\n",
      "Losses {'ner': 21.374954189912387}\n",
      "Losses {'ner': 25.599431089864083}\n",
      "Losses {'ner': 27.907420095794805}\n",
      "Losses {'ner': 29.99299375647024}\n",
      "Losses {'ner': 33.53832326647171}\n",
      "Losses {'ner': 35.454518112836695}\n",
      "Losses {'ner': 41.51721722960315}\n",
      "Losses {'ner': 47.02452853207319}\n",
      "Losses {'ner': 56.54380684976309}\n",
      "Losses {'ner': 5.494173554121517}\n",
      "Losses {'ner': 11.150969888898544}\n",
      "Losses {'ner': 14.265594551106915}\n",
      "Losses {'ner': 17.420677051646635}\n",
      "Losses {'ner': 22.89790870132856}\n",
      "Losses {'ner': 22.899538359801227}\n",
      "Losses {'ner': 29.790954149524623}\n",
      "Losses {'ner': 34.77616399453109}\n",
      "Losses {'ner': 37.52261371813802}\n",
      "Losses {'ner': 43.986880152449885}\n",
      "Losses {'ner': 48.25099575958302}\n",
      "Losses {'ner': 49.36916051219305}\n",
      "Losses {'ner': 52.285110241944494}\n",
      "Losses {'ner': 55.040121172765794}\n",
      "Losses {'ner': 4.880794237637019}\n",
      "Losses {'ner': 7.110386489228404}\n",
      "Losses {'ner': 9.57118558735965}\n",
      "Losses {'ner': 13.897776602221711}\n",
      "Losses {'ner': 16.390056222271596}\n",
      "Losses {'ner': 16.450525770584136}\n",
      "Losses {'ner': 22.306033982584268}\n",
      "Losses {'ner': 29.155356614830453}\n",
      "Losses {'ner': 32.48490950032465}\n",
      "Losses {'ner': 32.53054919997703}\n",
      "Losses {'ner': 33.50405968133513}\n",
      "Losses {'ner': 38.798406087460116}\n",
      "Losses {'ner': 45.96774337657693}\n",
      "Losses {'ner': 52.15668033856631}\n",
      "Losses {'ner': 3.8968892409466207}\n",
      "Losses {'ner': 5.785664435056873}\n",
      "Losses {'ner': 5.80089362056728}\n",
      "Losses {'ner': 9.839665625328983}\n",
      "Losses {'ner': 14.950302456363833}\n",
      "Losses {'ner': 20.484490272447264}\n",
      "Losses {'ner': 27.389155462541456}\n",
      "Losses {'ner': 31.88706321657526}\n",
      "Losses {'ner': 39.23498002827989}\n",
      "Losses {'ner': 41.05406958052197}\n",
      "Losses {'ner': 46.556023216485855}\n",
      "Losses {'ner': 51.281265272113856}\n",
      "Losses {'ner': 54.399002259102744}\n",
      "Losses {'ner': 57.823378037347865}\n",
      "Losses {'ner': 6.90338647304452}\n",
      "Losses {'ner': 6.908897951057952}\n",
      "Losses {'ner': 7.906154552819771}\n",
      "Losses {'ner': 12.232704701100374}\n",
      "Losses {'ner': 17.906319914583946}\n",
      "Losses {'ner': 20.80990880041759}\n",
      "Losses {'ner': 22.51032930739202}\n",
      "Losses {'ner': 25.480740218107165}\n",
      "Losses {'ner': 28.411562092704024}\n",
      "Losses {'ner': 35.328457944196906}\n",
      "Losses {'ner': 40.62749527619726}\n",
      "Losses {'ner': 42.696563928239264}\n",
      "Losses {'ner': 43.70061143938474}\n",
      "Losses {'ner': 47.15124267476167}\n",
      "Losses {'ner': 7.914743488188833}\n",
      "Losses {'ner': 13.148951573297381}\n",
      "Losses {'ner': 21.650046050432138}\n",
      "Losses {'ner': 25.066720989018904}\n",
      "Losses {'ner': 31.408860233098494}\n",
      "Losses {'ner': 35.51068089781165}\n",
      "Losses {'ner': 41.323312909061315}\n",
      "Losses {'ner': 47.65044103441596}\n",
      "Losses {'ner': 55.12518404302955}\n",
      "Losses {'ner': 61.0880406263268}\n",
      "Losses {'ner': 65.6240027944541}\n",
      "Losses {'ner': 70.03323117894888}\n",
      "Losses {'ner': 73.78575906781316}\n",
      "Losses {'ner': 76.24197476541727}\n",
      "Losses {'ner': 0.0036683415785816464}\n",
      "Losses {'ner': 3.290935833583994}\n",
      "Losses {'ner': 6.9223227630230255}\n",
      "Losses {'ner': 12.712101919469482}\n",
      "Losses {'ner': 15.47115576209972}\n",
      "Losses {'ner': 21.830935284271895}\n",
      "Losses {'ner': 26.022062983547862}\n",
      "Losses {'ner': 29.41573547543731}\n",
      "Losses {'ner': 31.60420615333947}\n",
      "Losses {'ner': 34.76250513750943}\n",
      "Losses {'ner': 43.14568051058682}\n",
      "Losses {'ner': 43.35475334432936}\n",
      "Losses {'ner': 49.00245701580803}\n",
      "Losses {'ner': 51.10892506030955}\n",
      "Losses {'ner': 5.0155201852321625}\n",
      "Losses {'ner': 11.5667763678357}\n",
      "Losses {'ner': 18.29324353300035}\n",
      "Losses {'ner': 22.117610230052378}\n",
      "Losses {'ner': 24.360264523827937}\n",
      "Losses {'ner': 26.222728423032095}\n",
      "Losses {'ner': 29.994264544176986}\n",
      "Losses {'ner': 35.43010269985825}\n",
      "Losses {'ner': 43.818243139845436}\n",
      "Losses {'ner': 49.83067839728028}\n",
      "Losses {'ner': 53.12718427963409}\n",
      "Losses {'ner': 53.381487759666925}\n",
      "Losses {'ner': 59.66611057789851}\n",
      "Losses {'ner': 63.41951019870612}\n",
      "Losses {'ner': 4.379199825227261}\n",
      "Losses {'ner': 7.635098187252879}\n",
      "Losses {'ner': 8.654337289743125}\n",
      "Losses {'ner': 12.118801629985683}\n",
      "Losses {'ner': 13.99686682221909}\n",
      "Losses {'ner': 19.707263848406}\n",
      "Losses {'ner': 22.171621061521364}\n",
      "Losses {'ner': 27.305083736437155}\n",
      "Losses {'ner': 32.62155861187239}\n",
      "Losses {'ner': 36.8949225632623}\n",
      "Losses {'ner': 36.90246237845167}\n",
      "Losses {'ner': 42.24098329276785}\n",
      "Losses {'ner': 46.066432267734854}\n",
      "Losses {'ner': 48.439831514684556}\n"
     ]
    }
   ],
   "source": [
    "# Importing requirements\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "\n",
    "# Begin training by disabling other pipeline components\n",
    "with nlp.disable_pipes(*other_pipes) :\n",
    "\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    # Training for 30 iterations     \n",
    "    for itn in range(30):\n",
    "        # shuffle examples before training\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "  \n",
    "      # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "        # ictionary to store losses\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "              # Calling update() over the iteration\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'I ate Sushi yesterday. Maggi is a common fast food '\n",
      "I\n",
      "Maggi\n"
     ]
    }
   ],
   "source": [
    "# Testing the NER\n",
    "\n",
    "test_text = \"I ate Sushi yesterday. Maggi is a common fast food \"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in a Output directory\n",
    "from pathlib import Path\n",
    "output_dir=Path('/content/')\n",
    "\n",
    "# Saving the model to the output directory\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "nlp.meta['name'] = 'my_ner'  # rename model\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)\n",
    "\n",
    "# Loading the model from the directory\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "assert nlp2.get_pipe(\"ner\").move_names == move_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(' Dosa is an extremely famous south Indian dish')\n",
    "for ent in doc2.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/\n",
    "https://www.machinelearningplus.com/spacy-tutorial-nlp/#creatingcustompipelinecomponents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
