{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/deepakm/mywork/mynlp/txnstmt'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kde_id</th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>transactionDate</th>\n",
       "      <th>transactionAmount</th>\n",
       "      <th>transactionAmountCurrency</th>\n",
       "      <th>bankTransactionFlowType</th>\n",
       "      <th>currentBalanceAmount</th>\n",
       "      <th>currentBalanceCurrency</th>\n",
       "      <th>accountIdentifier</th>\n",
       "      <th>bankIdentifier</th>\n",
       "      <th>amountInFunctionalCurrencyAmount</th>\n",
       "      <th>amountInFunctionalCurrencyCurrency</th>\n",
       "      <th>currentBalanceInFunctionalCurrencyAmount</th>\n",
       "      <th>currentBalanceInFunctionalCurrencyCurrency</th>\n",
       "      <th>bankDataSource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10048</td>\n",
       "      <td>1</td>\n",
       "      <td>NL</td>\n",
       "      <td>AUTOMAT S6EE6080 0063 Karte3 27.06.2015 UM 18:...</td>\n",
       "      <td>2015-06-29</td>\n",
       "      <td>-400.00</td>\n",
       "      <td>EUR</td>\n",
       "      <td>OUTFLOW</td>\n",
       "      <td>-6006.540039</td>\n",
       "      <td>EUR</td>\n",
       "      <td>170331</td>\n",
       "      <td>Bawag</td>\n",
       "      <td>-400.00</td>\n",
       "      <td>EUR</td>\n",
       "      <td>-6006.540039</td>\n",
       "      <td>EUR</td>\n",
       "      <td>titanium-db</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10048</td>\n",
       "      <td>2</td>\n",
       "      <td>NL</td>\n",
       "      <td>---- 0103619245 / 200003507629 ----Rechnungsnr...</td>\n",
       "      <td>2015-06-29</td>\n",
       "      <td>-38.08</td>\n",
       "      <td>EUR</td>\n",
       "      <td>OUTFLOW</td>\n",
       "      <td>-6406.540039</td>\n",
       "      <td>EUR</td>\n",
       "      <td>170331</td>\n",
       "      <td>Bawag</td>\n",
       "      <td>-38.08</td>\n",
       "      <td>EUR</td>\n",
       "      <td>-6406.540039</td>\n",
       "      <td>EUR</td>\n",
       "      <td>titanium-db</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10048</td>\n",
       "      <td>257</td>\n",
       "      <td>NL</td>\n",
       "      <td>150351</td>\n",
       "      <td>2015-06-29</td>\n",
       "      <td>228.64</td>\n",
       "      <td>EUR</td>\n",
       "      <td>INFLOW</td>\n",
       "      <td>-48527.500000</td>\n",
       "      <td>EUR</td>\n",
       "      <td>170511</td>\n",
       "      <td>Bawag</td>\n",
       "      <td>228.64</td>\n",
       "      <td>EUR</td>\n",
       "      <td>-48527.500000</td>\n",
       "      <td>EUR</td>\n",
       "      <td>titanium-db</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10048</td>\n",
       "      <td>258</td>\n",
       "      <td>NL</td>\n",
       "      <td>Kundennummer 000000086189, Hutchison Drei Aust...</td>\n",
       "      <td>2015-06-29</td>\n",
       "      <td>-30.22</td>\n",
       "      <td>EUR</td>\n",
       "      <td>OUTFLOW</td>\n",
       "      <td>-48298.859380</td>\n",
       "      <td>EUR</td>\n",
       "      <td>170511</td>\n",
       "      <td>Bawag</td>\n",
       "      <td>-30.22</td>\n",
       "      <td>EUR</td>\n",
       "      <td>-48298.859380</td>\n",
       "      <td>EUR</td>\n",
       "      <td>titanium-db</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10048</td>\n",
       "      <td>259</td>\n",
       "      <td>NL</td>\n",
       "      <td>---- 0103619245 / 200003768285 ----Rechnungsnr...</td>\n",
       "      <td>2015-06-29</td>\n",
       "      <td>-34.98</td>\n",
       "      <td>EUR</td>\n",
       "      <td>OUTFLOW</td>\n",
       "      <td>-48329.078130</td>\n",
       "      <td>EUR</td>\n",
       "      <td>170511</td>\n",
       "      <td>Bawag</td>\n",
       "      <td>-34.98</td>\n",
       "      <td>EUR</td>\n",
       "      <td>-48329.078130</td>\n",
       "      <td>EUR</td>\n",
       "      <td>titanium-db</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kde_id   id country                                        description  \\\n",
       "0   10048    1      NL  AUTOMAT S6EE6080 0063 Karte3 27.06.2015 UM 18:...   \n",
       "1   10048    2      NL  ---- 0103619245 / 200003507629 ----Rechnungsnr...   \n",
       "2   10048  257      NL                                           150351     \n",
       "3   10048  258      NL  Kundennummer 000000086189, Hutchison Drei Aust...   \n",
       "4   10048  259      NL  ---- 0103619245 / 200003768285 ----Rechnungsnr...   \n",
       "\n",
       "  transactionDate  transactionAmount transactionAmountCurrency  \\\n",
       "0      2015-06-29            -400.00                       EUR   \n",
       "1      2015-06-29             -38.08                       EUR   \n",
       "2      2015-06-29             228.64                       EUR   \n",
       "3      2015-06-29             -30.22                       EUR   \n",
       "4      2015-06-29             -34.98                       EUR   \n",
       "\n",
       "  bankTransactionFlowType  currentBalanceAmount currentBalanceCurrency  \\\n",
       "0                 OUTFLOW          -6006.540039                    EUR   \n",
       "1                 OUTFLOW          -6406.540039                    EUR   \n",
       "2                  INFLOW         -48527.500000                    EUR   \n",
       "3                 OUTFLOW         -48298.859380                    EUR   \n",
       "4                 OUTFLOW         -48329.078130                    EUR   \n",
       "\n",
       "   accountIdentifier bankIdentifier  amountInFunctionalCurrencyAmount  \\\n",
       "0             170331          Bawag                           -400.00   \n",
       "1             170331          Bawag                            -38.08   \n",
       "2             170511          Bawag                            228.64   \n",
       "3             170511          Bawag                            -30.22   \n",
       "4             170511          Bawag                            -34.98   \n",
       "\n",
       "  amountInFunctionalCurrencyCurrency  \\\n",
       "0                                EUR   \n",
       "1                                EUR   \n",
       "2                                EUR   \n",
       "3                                EUR   \n",
       "4                                EUR   \n",
       "\n",
       "   currentBalanceInFunctionalCurrencyAmount  \\\n",
       "0                              -6006.540039   \n",
       "1                              -6406.540039   \n",
       "2                             -48527.500000   \n",
       "3                             -48298.859380   \n",
       "4                             -48329.078130   \n",
       "\n",
       "  currentBalanceInFunctionalCurrencyCurrency bankDataSource  \n",
       "0                                        EUR    titanium-db  \n",
       "1                                        EUR    titanium-db  \n",
       "2                                        EUR    titanium-db  \n",
       "3                                        EUR    titanium-db  \n",
       "4                                        EUR    titanium-db  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtxn=pd.read_csv('flat_transactions.csv')\n",
    "dtxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AUTOMAT S6EE6080 0063 Karte3 27.06.2015 UM 18:22  ',\n",
       " '---- 0103619245 / 200003507629 ----Rechnungsnr. 295114742 120 ',\n",
       " '150351  ',\n",
       " 'Kundennummer 000000086189, Hutchison Drei Austria  ',\n",
       " '---- 0103619245 / 200003768285 ----Rechnungsnr. 295114744 732 ',\n",
       " 'Rg.Nr. 150390  ',\n",
       " 'R 150352  ',\n",
       " 'Renr: 150402      /KR-Nr: 002910-00000-00/RE-Nr:   201500 0583/Rein. Ba                                   lkon Wies enhofer Ultra',\n",
       " 'Renr: 150403      /KR-Nr: 002980-00000-00/RE-Nr:   201500 0499/Mont. Ba                                   nk, Ultra s',\n",
       " 'Renr: 150406      /KR-Nr: 007870-00448-65/RE-Nr:   201500 1533/Sanierun                                   g Zufahrt sstra\\x9de',\n",
       " 'Renr: 150400      /KR-Nr: 006860-00000-00/RE-Nr:   201500 1508/Ultras Z                                   ylinder v ermessenWhg11',\n",
       " 'Kd.Nr:234545 Re.Nr:151119  ',\n",
       " '  ',\n",
       " '  ',\n",
       " '  ',\n",
       " '  ',\n",
       " 'Entgelt f\\x9dr Kontof\\x9dhrung  ',\n",
       " 'Entgelt Kontoauszug                                1 Stk.  ',\n",
       " 'Entgelt f\\x9dr automatisierte Buchungen              66 Stk.  ',\n",
       " 'Entgelt f\\x9dr h\\x9dnd. bearbeitete Buchungen            1 Stk.  ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=dtxn['description'].tolist()\n",
    "print(len(data))\n",
    "data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings from Universal Sentence Encoder\n",
    "https://tfhub.dev/google/universal-sentence-encoder-large/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "# embed=hub.load(\"/pretrained_models/universal-sentence-encoder_4\")\n",
    "embeddings=embed(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings from BERT models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit#gid=0\n",
    "roberta-large-nli-stsb-mean-tokens\n",
    "roberta-base-nli-stsb-mean-tokens\n",
    "bert-large-nli-stsb-mean-tokens\n",
    "distilbert-base-nli-stsb-mean-tokens\n",
    "bert-base-nli-stsb-mean-tokens\n",
    "bert-base-nli-stsb-wkpooling\n",
    "distilbert-base-nli-stsb-wkpooling\n",
    "xlm-r-paraphrase-v1.\n",
    "distilroberta-base-paraphrase-v1\n",
    "distilbert-base-nli-wkpooling\n",
    "distilbert-base-nli-mean-tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models tuned for cosine-similarity will prefer the retrieval of short documents, \n",
    "#### while models tuned for dot-product will prefer the retrieval of longer documents. \n",
    "#### Depending on your task, the models of the one or the other type are preferable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15f7e3f3f4147a093ab154e0096b08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=391.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed1c06ee5c2419da40d7dfb60813a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3931.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fea3ee5e374d86896dc9b63811d33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1eb6fe72c349d3a1f994e43b10b004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e316d9c3fe9e4074b9a95ab4640bb36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=122.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d1065624944c0d9df419ce3e8ca567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=229.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b2664ab5cd4166bbd7f5ac653d70fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=438007537.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93883ee73f7c40cca809640c85c715e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=53.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad04001d5fd84b6bb31d478cdf5f0e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9086feac71d240e4ab52fac538175ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466081.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb391c9fa864bbe8ed202772aa74d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=399.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4de5a0872e4656be25f1681bf20362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a0f0221d054e4f96a928770584ebe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=190.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentence_transformers\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('msmarco-distilbert-base-v3')\n",
    "\n",
    "query_embedding = model.encode('How big is London')\n",
    "passage_embedding = model.encode('London has 9,787,426 inhabitants at the 2011 census')\n",
    "\n",
    "print(\"Similarity:\", util.pytorch_cos_sim(query_embedding, passage_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = model.encode(data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REDUCE DIMENSIONS OF EMBEDDINGS FOR BETTER CLUSTERING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "umap_embeddings = UMAP(n_neighbors=10,n_components=20, metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer(copy=False)\n",
    "umap_embeddings=normalizer.fit_transform(umap_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLUSTER THE RESULTS\n",
    "https://nbviewer.jupyter.org/github/scikit-learn-contrib/hdbscan/blob/master/notebooks/How%20HDBSCAN%20Works.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering by HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN,RobustSingleLinkage\n",
    "clusterer = HDBSCAN(min_cluster_size=15,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)\n",
    "print(\"Number of Clusters found: \",clusterer.labels_.max())\n",
    "print(\"Cluster Probabilities: \",clusterer.probabilities_)\n",
    "# print(\"Cluster Validity: \",clusterer.relative_validity_)\n",
    "# clusterer = RobustSingleLinkage(cut=0.125, k=7)\n",
    "# cluster_labels = clusterer.fit_predict(data)\n",
    "\n",
    "# hierarchy = clusterer.cluster_hierarchy_\n",
    "# alt_labels = hierarchy.get_clusters(0.100, 5)\n",
    "# hierarchy.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT THE RESULTS BY REDUCING DIM TO 2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data\n",
    "umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "result['labels'] = clusterer.labels_\n",
    "\n",
    "# Visualize clusters\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Text by Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(data, columns=[\"Doc\"])\n",
    "docs_df['Clust'] = clusterer.labels_\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "print(docs_df.head())\n",
    "docs_per_cluster = docs_df.groupby(['Clust'], as_index = False).agg({'Doc': ' '.join})\n",
    "docs_per_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOP N WORDS IN CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_n_words_per_cluster(tf_idf, count, docs_per_cluster, n=10):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_cluster.Clust)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_cluster_sizes(df):\n",
    "    cluster_sizes = (df.groupby(['Clust'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Clust\": \"Clust\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return cluster_sizes\n",
    "\n",
    "top_n_words = extract_top_n_words_per_cluster(tf_idf, count, docs_per_cluster, n=20)\n",
    "cluster_sizes = extract_cluster_sizes(docs_df)\n",
    "print(\"Number of Clusters Found: \",cluster_sizes.shape[0])\n",
    "cluster_sizes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words[1][:10]\n",
    "top_n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reductions of Cluster by Combining Similar Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "for i in range(20):\n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(tf_idf.T)\n",
    "    np.fill_diagonal(similarities, 0)\n",
    "\n",
    "    # Extract label to merge into and from where\n",
    "    topic_sizes = docs_df.groupby(['Clust']).count().sort_values(\"Doc\", ascending=False).reset_index()\n",
    "    topic_to_merge = topic_sizes.iloc[-1].Clust\n",
    "    topic_to_merge_into = np.argmax(similarities[topic_to_merge + 1]) - 1\n",
    "\n",
    "    # Adjust topics\n",
    "    docs_df.loc[docs_df.Clust == topic_to_merge, \"Clust\"] = topic_to_merge_into\n",
    "    old_topics = docs_df.sort_values(\"Clust\").Clust.unique()\n",
    "    map_topics = {old_topic: index - 1 for index, old_topic in enumerate(old_topics)}\n",
    "    docs_df.Topic = docs_df.Clust.map(map_topics)\n",
    "    docs_per_topic = docs_df.groupby(['Clust'], as_index = False).agg({'Doc': ' '.join})\n",
    "\n",
    "    # Calculate new topic words\n",
    "    m = len(data)\n",
    "    tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m)\n",
    "    top_n_words = extract_top_n_words_per_cluster(tf_idf, count, docs_per_topic, n=10)\n",
    "\n",
    "topic_sizes = extract_cluster_sizes(docs_df)\n",
    "print(\"Number of Topics after Reduction: \",topic_sizes.shape[0])\n",
    "topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words[-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topw={}\n",
    "for k,v in top_n_words.items():\n",
    "    topw[k]=[i[0] for i in v]\n",
    "\n",
    "topicdt=pd.DataFrame.from_dict(topw).T.reset_index()\n",
    "topicdt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicdt.to_csv(\"topicdt_transf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=umap_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WE WILL USE K MEANS CLUSTERING ON DESCRIPTIONS\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FIND OUT OPTIMAL NUMBER OF CLUSTERS\n",
    "#METHOD 1: Elbow curve based on SSE \n",
    "sse = {}\n",
    "slh={}\n",
    "for k in range(2, 50):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X)\n",
    "    sse[k] = kmeans.inertia_ \n",
    "#    this is to find silhouette score\n",
    "    label = kmeans.labels_\n",
    "    slh[k] = silhouette_score(X, label, metric='euclidean')\n",
    "#PLOT THE GRAPHS\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(list(sse.keys()), list(sse.values()))\n",
    "plt.xlabel(\"Number of cluster\")\n",
    "plt.ylabel(\"SSE\")\n",
    "# plt.show()\n",
    "#METHOD 2: Using  silhouette score: higher the better\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(list(slh.keys()), list(slh.values()))\n",
    "plt.xlabel(\"Number of cluster\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "# plt.show()\n",
    "\n",
    "num_clus=max(slh,key=slh.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#GET num_clus FROM ABOVE 2 METHODS AND FIT THE FINAL CLSUTER\n",
    "num_clus = 20\n",
    "model = KMeans(n_clusters=num_clus, init='k-means++', max_iter=100, n_init=2)\n",
    "model.fit(X)\n",
    "cluster_assignment = model.labels_\n",
    "clustered_sentences = [[] for i in range(num_clus)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(ques[sentence_id])\n",
    "\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(\"Cluster \", i+1)\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_data={}\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    clustered_data[i+1]=cluster\n",
    "\n",
    "\n",
    "# clustered_data\n",
    "clustered_data=pd.DataFrame.from_dict(clustered_data,orient = 'index').T\n",
    "clustered_data.columns=[\"Clus_\"+str(col) for col in clustered_data.columns]\n",
    "clustered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.cluster as cluster\n",
    "import time\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}\n",
    "def plot_clusters(data, algorithm, args, kwds):\n",
    "    start_time = time.time()\n",
    "    labels = algorithm(*args, **kwds).fit_predict(data)\n",
    "    end_time = time.time()\n",
    "    palette = sns.color_palette('deep', np.unique(labels).max() + 1)\n",
    "    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
    "    plt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds)\n",
    "    frame = plt.gca()\n",
    "    frame.axes.get_xaxis().set_visible(False)\n",
    "    frame.axes.get_yaxis().set_visible(False)\n",
    "    plt.title('Clusters found by {}'.format(str(algorithm.__name__)), fontsize=24)\n",
    "    plt.text(-0.5, 0.7, 'Clustering took {:.2f} s'.format(end_time - start_time), fontsize=14)\n",
    "\n",
    "plot_clusters(umap_embeddings, cluster.KMeans, (), {'n_clusters':20})\n",
    "\n",
    "# plot_clusters(umap_embeddings, cluster.AffinityPropagation, (), {'preference':-5.0, 'damping':0.95})\n",
    "# plot_clusters(umap_embeddings, cluster.MeanShift, (0.175,), {'cluster_all':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    " \n",
    "# data = fetch_20newsgroups(subset='all')['data']\n",
    "\n",
    "model = BERTopic(\"distilbert-base-nli-stsb-mean-tokens\", verbose=True)\n",
    "topics, probabilities = model.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.get_topic(1)\n",
    "pd.DataFrame.from_dict(model.get_topics()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to input the probabilities of a single document!\n",
    "model.visualize_distribution(probabilities[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, RobertaTokenizer\n",
    "distil_bert = 'distilbert-base-uncased' # Pick any desired pre-trained model\n",
    "# roberta = 'roberta-base-uncase'\n",
    "\n",
    "# Defining DistilBERT tokonizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True,max_length=128, pad_to_max_length=True)\n",
    "# Defining RoBERTa tokinizer\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(roberta, do_lower_case=True, add_special_tokens=True,max_length=128, pad_to_max_length=True)\n",
    "\n",
    "\n",
    "def tokenize(sentences, tokenizer):\n",
    "    input_ids, input_masks, input_segments = [],[],[]\n",
    "    for sentence in tqdm(sentences):\n",
    "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=128, pad_to_max_length=True, \n",
    "                                             return_attention_mask=True, return_token_type_ids=True)\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "        input_segments.append(inputs['token_type_ids'])        \n",
    "        \n",
    "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n",
    "\n",
    "\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig\n",
    "import tensorflow as tf\n",
    "\n",
    "distil_bert = 'distilbert-base-uncased'\n",
    "\n",
    "config = DistilBertConfig(num_labels=6)\n",
    "config.output_hidden_states = False\n",
    "transformer_model = TFDistilBertForSequenceClassification.from_pretrained(distil_bert, config = config)[0]\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
    "input_masks_ids = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32')\n",
    "X = transformer_model(input_ids, input_masks_ids)\n",
    "model = tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs = X)\n",
    "\n",
    "\n",
    "\n",
    "distil_bert = 'distilbert-base-uncased'\n",
    "\n",
    "config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "config.output_hidden_states = False\n",
    "transformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)\n",
    "\n",
    "input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
    "input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
    "\n",
    "embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n",
    "cls_token = embedding_layer[:,0,:]\n",
    "X = tf.keras.layers.BatchNormalization()(cls_token)\n",
    "X = tf.keras.layers.Dense(192, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.2)(X)\n",
    "X = tf.keras.layers.Dense(6, activation='softmax')(X)\n",
    "model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
    "\n",
    "for layer in model.layers[:3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "distil_bert = 'distilbert-base-uncased'\n",
    "\n",
    "config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "config.output_hidden_states = False\n",
    "transformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)\n",
    "\n",
    "input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
    "input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
    "\n",
    "embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n",
    "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
    "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
    "X = tf.keras.layers.Dense(50, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.2)(X)\n",
    "X = tf.keras.layers.Dense(6, activation='sigmoid')(X)\n",
    "model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
    "\n",
    "for layer in model.layers[:3]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "498px",
    "left": "1181px",
    "right": "20px",
    "top": "120px",
    "width": "322px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
